import os
import textract
from langchain.langchain import LangChain
import openai
from cachetools import LRUCache

# Set up your LangChain instance
langchain = LangChain()

# Set up your OpenAI API credentials
openai.api_key = 'YOUR_OPENAI_API_KEY'

# Function to read the document content based on file format
def read_document(file_path):
    file_extension = os.path.splitext(file_path)[1].lower()
    text = ""

    if file_extension == ".docx":
        try:
            import docx2txt
            text = docx2txt.process(file_path)
        except ImportError:
            print("Please install the python-docx module to read .docx files.")
    elif file_extension == ".pdf":
        try:
            text = textract.process(file_path).decode('utf-8')
        except ImportError:
            print("Please install the textract module to read .pdf files.")
    elif file_extension == ".txt":
        try:
            with open(file_path, 'r') as file:
                text = file.read()
        except FileNotFoundError:
            print("File not found.")

    return text

# Function to generate document embeddings
def generate_document_embeddings(document):
    # Generate document embeddings using LangChain
    embeddings = langchain.process(document, 'embeddings')
    return embeddings

# Function to summarize a document
def summarize_document(document_embeddings):
    # Generate a summary using LangChain
    summary = langchain.process(document_embeddings, 'summary')
    return summary

# Function to answer user questions
def answer_question(question_embeddings, document_embeddings):
    # Generate an answer using LangChain and OpenAI GPT-3
    context = f'Question Embeddings: {question_embeddings}\nDocument Embeddings: {document_embeddings}\nAnswer:'
    answer = langchain.process(context, 'answer')

    # Return the answer generated by GPT-3
    return answer

# Example usage
file_path = 'path/to/your/file.docx'
document = read_document(file_path)
document_embeddings = generate_document_embeddings(document)
summary = summarize_document(document_embeddings)
print("Summary:", summary)

# Set up buffer memory using an LRU cache
session_data = LRUCache(maxsize=10)  # Buffer memory for 10 sessions

# Interactive question-answering loop
while True:
    user_question = input("Ask a question (or 'exit' to quit): ")
    if user_question.lower() == 'exit':
        break

    # Check if session data exists for the current user session
    if 'document_embeddings' in session_data:
        question_embeddings = langchain.process(user_question, 'embeddings')
        answer = answer_question(question_embeddings, session_data['document_embeddings'])
        print("Answer:", answer)
    else:
        print("Please provide the document before asking questions.")
        session_data['document_embeddings'] = document_embeddings

    # Clear session data of older sessions when a new session starts
    if len(session_data) > session_data.maxsize:
        session_data.popitem(last=False)